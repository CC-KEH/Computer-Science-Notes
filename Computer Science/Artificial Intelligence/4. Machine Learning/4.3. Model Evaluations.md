## **Evaluation of Regression Models**

#### **(i). Mean Absolute Error (MAE)**

- Measures the average absolute difference between predicted and actual values:
$$MAE = \frac{1}{n} \sum_{i=1}^n |Y_i - \hat{Y}_i|$$
- Less sensitive to outliers.
- **In simple terms**: Tells how far off predictions are, on average.

#### **(ii). Mean Squared Error (MSE)**

 - Penalizes larger errors more than smaller ones:
 $$MSE = \frac{1}{n} \sum_{i=1}^n (Y_i - \hat{Y}_i)^2$$
 - Sensitive to outliers.
 - **In simple terms**: Emphasizes large prediction errors.

#### **(iii). Root Mean Squared Error (RMSE)**

- The square root of MSE, providing error in the same units as the target variable:
$$RMSE = \sqrt{\frac{1}{n} \sum_{i=1}^n (Y_i - \hat{Y}_i)^2}$$
- Highlights larger errors more prominently.
- **In simple terms**: Gives an overall sense of prediction error size.

#### **(iv). R-squared**

- Measures the proportion of variance in the target variable explained by the model:
$$R^2 = 1 - \frac{\text{SS}_{res}}{\text{SS}_{tot}}$$ 
Here, $\text{SS}_{res}$ is the residual sum of squares, and $\text{SS}_{tot}$ is the total sum of squares.
- **In simple terms**: Shows how well the model explains the data.

---

## **When to Use Which Metric?**

| **Scenario**                                                                               | **Best Metric** |
| ------------------------------------------------------------------------------------------ | --------------- |
| **General error measurement with equal weight to all errors**                              | MAE             |
| **When large errors need to be penalized more**                                            | MSE or RMSE     |
| **Comparing models with different numbers of features**                                    | Adjusted R²     |
| **Evaluating how well the model explains variance**                                        | R² Score        |
| **When percentage errors are more meaningful (e.g., finance, sales forecasting)**          | MAPE            |
| **When under-predictions should be penalized more (e.g., predicting house prices, sales)** | MSLE            |
| **When the target variable has large values and needs scale-invariant comparison**         | RMSE            |

****

## **Evaluation of Classification Models**

The **Confusion Matrix** is a table used to evaluate the performance of a classification model. It shows the comparison between actual and predicted labels.

- **True Positive (TP)**: Model correctly predicted **Positive**.
- **False Positive (FP)**: Model incorrectly predicted **Positive** (Type I Error).
- **False Negative (FN)**: Model incorrectly predicted **Negative** (Type II Error).
- **True Negative (TN)**: Model correctly predicted **Negative**.

---

## **Performance Metrics (Classification Scores)**

#### **(i) Accuracy**

$$Accuracy = \frac{TP + TN}{TP + TN + FP + FN}$$

- Measures the proportion of correct predictions.
- **Limitation**: Misleading if classes are imbalanced.

#### **(ii) Precision (Positive Predictive Value)**

$$Precision = \frac{TP}{TP + FP}$$

- Out of all predicted **positives**, how many are actually **positive**?
- **Important when False Positives are costly** (e.g., Spam detection).

#### **(iii) Recall (Sensitivity / True Positive Rate)**

$$Recall = \frac{TP}{TP + FN}$$

- Out of all **actual positives**, how many were correctly predicted?
- **Important when False Negatives are costly** (e.g., Cancer diagnosis).

#### **(iv) Specificity (True Negative Rate)**

$$Specificity = \frac{TN}{TN + FP}$$

- Measures the proportion of actual **negatives** correctly identified.
- **Useful in medical tests** (ensures not too many false positives).

#### **(v) F1-Score**

$$F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$$

- Harmonic mean of **Precision** and **Recall**.
- **Balances False Positives & False Negatives**.
- **Best when dataset is imbalanced**.

#### **(vi) False Positive Rate (FPR)**

$$FPR = \frac{FP}{FP + TN} = 1 - \text{Specificity}$$

- Measures how many **actual negatives** were incorrectly classified as **positives**.
- **Lower is better** (important in fraud detection).

#### **(vii) False Negative Rate (FNR)**

$$FNR = \frac{FN}{TP + FN} = 1 - \text{Recall}$$

- Measures how many **actual positives** were missed.

---

## **Receiver Operating Characteristic (ROC) Curve**

- The **ROC Curve** is a graphical representation of a model’s performance across different classification thresholds.
- **X-axis**: **False Positive Rate (FPR)**
- **Y-axis**: **True Positive Rate (TPR) = Recall**
- **Key Concept**: As the threshold decreases, **more positives are predicted**, increasing **TPR but also FPR**.

**Interpretation**:

- **Ideal Model**: Steep curve reaching **(0,1) quickly**.
- **Random Model**: Diagonal line (no discrimination ability).
- **Bad Model**: Below diagonal (worse than random guessing).

---

## **Area Under the Curve (AUC)**

- **AUC (Area Under the ROC Curve)** quantifies the overall ability of the model to distinguish between classes.
- **Higher AUC (closer to 1) → Better Model.**
- **Interpretation of AUC Values**:
    - **AUC = 1** → Perfect classification.
    - **0.9 ≤ AUC < 1** → Excellent classification.
    - **0.8 ≤ AUC < 0.9** → Good classification.
    - **0.7 ≤ AUC < 0.8** → Fair classification.
    - **AUC = 0.5** → No discrimination (random guessing).

---

## **Precision-Recall (PR) Curve**

- **Used when data is highly imbalanced**.
- **X-axis**: Recall
- **Y-axis**: Precision
- **PR AUC** is an alternative to ROC-AUC when dealing with imbalanced data.

---

## **When to Use Which Metric?**

|**Scenario**|**Best Metric**|
|---|---|
|**Balanced dataset**|Accuracy|
|**Imbalanced dataset**|F1-score / PR-AUC|
|**Minimizing False Positives (e.g., spam detection)**|Precision|
|**Minimizing False Negatives (e.g., cancer detection)**|Recall|
|**Overall model performance**|AUC-ROC|

---

# **Summary**

Model evaluation is crucial in assessing how well a machine learning model performs, with metrics tailored to the type of problem—regression or classification. For regression models, common metrics include Mean Absolute Error (MAE) for average error, Mean Squared Error (MSE) and Root Mean Squared Error (RMSE) which penalize larger errors more heavily, and R² Score to quantify how much of the target variance is explained by the model. These metrics serve different needs: MAE for general error, MSE/RMSE when large errors matter more, and R² for model explainability. On the classification side, performance is often evaluated using the Confusion Matrix and derived metrics such as Accuracy, Precision, Recall, Specificity, and F1-Score, each offering insights into different error types like False Positives and False Negatives. Additionally, the ROC Curve and its Area Under the Curve (AUC) indicate a model’s discriminative power, while the Precision-Recall Curve is particularly useful in imbalanced datasets.

---
