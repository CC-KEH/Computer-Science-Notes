# Unsupervised Learning

Unsupervised learning is a machine learning technique that uses algorithms to learn from unlabeled data without human intervention. It's well-suited for complex tasks like organizing data into clusters and finding patterns.

## **Unsupervised Algorithms**

#### **1. Principal Component Analysis (PCA)**

 Component Analysis (PCA) is a dimensionality reduction technique used to reduce the number of features in a dataset while preserving as much of the variance (information) as possible. It works by transforming the data into a new coordinate system, where the axes (principal components) are ordered by the amount of variance they capture. The goal is to project the data onto a lower-dimensional space while maintaining the most important features of the data. PCA was developed by **Karl Pearson** in 1901 as a method to identify patterns in data by transforming it into a new set of coordinates, known as eigenvectors. The modern algorithm was formalized by **Harold Hotelling** in 1933.

##### **Mathematical intuition:** 

PCA works by computing the **covariance matrix** of the data and finding its **eigenvectors** and **eigenvalues**. The eigenvectors (principal components) represent the directions in which the data varies the most, while the eigenvalues indicate the amount of variance captured by each eigenvector.

**Steps of PCA**:
1. **Standardize** the data (mean-center it and scale it if needed).
2. Compute the **covariance matrix** to understand how the features relate to each other.
3. Calculate the **eigenvectors** and **eigenvalues** of the covariance matrix. The eigenvectors determine the directions of maximum variance, and the eigenvalues tell you the magnitude of variance in each direction.
4. **Sort** the eigenvectors by their corresponding eigenvalues in descending order. The first few eigenvectors with the highest eigenvalues form the new basis (principal components).
5. **Project** the original data onto the new lower-dimensional space defined by the top kk principal components.

The data transformation is given by:
  $$Z = X W$$
where:
- $Z$ is the transformed data.
- $X$ is the original data matrix.
- $W$ is the matrix of selected eigenvectors (principal components).

##### **How it is used in regression**:

PCA is often used in regression problems to reduce the number of input features, thereby improving model performance and reducing overfitting. By transforming the data into a smaller set of principal components, the regression model can operate more efficiently, especially when there is multicollinearity or a high number of features.

**Example:** In predicting house prices based on many features (size, location, number of rooms, etc.), PCA can reduce the dimensionality of the feature space while preserving most of the variance in the data.

##### **How it is used in classification**: 

PCA is frequently used in classification tasks as a preprocessing step to reduce the dimensionality of the data before applying a classifier. By projecting the data into a lower-dimensional space, PCA helps simplify the problem and can improve the efficiency and performance of classifiers, especially with high-dimensional data.

**Example:** In face recognition, PCA can be used to reduce the number of pixels (features) while retaining the most important features of the images (such as the shape of the face).

##### **When to apply this**:

- When the dataset has many features (high dimensionality) and you want to reduce the complexity while retaining most of the important information.
- When there is multicollinearity (high correlation between features), as PCA can help eliminate redundancy by creating uncorrelated components.
- When the goal is to visualize data in lower dimensions (2D or 3D) for exploration or better understanding of the data.
- When you need to reduce noise or improve the performance of downstream algorithms by discarding less important features.
 
##### **Limitations**:

- PCA is sensitive to the scaling of the features, so it’s important to standardize the data if the features have different units or variances.
- PCA is a **linear** technique, meaning it may not perform well if the data has complex, non-linear relationships.
- The transformed features (principal components) may not be easily interpretable, as they are linear combinations of the original features.

---

#### **2. K-Means Clustering**

K-Means is a popular unsupervised learning algorithm used for clustering tasks. It groups a set of data points into **K** clusters, where each data point belongs to the cluster with the nearest mean. The algorithm iteratively assigns data points to clusters and updates the cluster centroids to minimize the within-cluster variance. K-Means was first introduced by **James MacQueen** in 1967. However, it was popularized and formalized in the 1980s as a widely used clustering algorithm.

##### **Mathematical intuition**: 

The objective of K-Means is to minimize the sum of squared distances between the data points and their respective cluster centroids. This is done by iteratively assigning points to the closest centroid and updating the centroids.

**Algorithm Steps**:
1. **Initialize**: Select $K$ initial centroids randomly or using a more advanced method like K-Means++.
2. **Assign**: Assign each data point to the closest centroid based on the Euclidean distance:
 
$$d(x, c_k) = \sqrt{(x_1 - c_{k1})^2 + (x_2 - c_{k2})^2 + \cdots + (x_n - c_{kn})^2}$$ where xx is a data point and $c_k$ is the centroid of cluster $k$.

3. **Update**: After all points are assigned, update the centroids by computing the mean of all data points assigned to each cluster: 
 
$$c_k = \frac{1}{N_k} \sum_{i \in C_k} x_i$$ where $N_k$ is the number of points in cluster $k$, and $C_k$ is the set of points in cluster $k$.

4. **Repeat**: Repeat the assign and update steps until convergence (when centroids no longer change significantly or a maximum number of iterations is reached).

##### **How it is used in regression**:

K-Means is not directly used for regression tasks, but it can be applied as a preprocessing step for **segmentation**. By grouping similar data points together, you can treat each cluster as a separate group for regression modeling, thus improving model performance by focusing on smaller, more homogeneous groups.

**Example:** If you're predicting house prices, K-Means can group houses into clusters based on features like location and size, and then separate regression models can be applied to each cluster to make more accurate predictions.

##### **How it is used in classification**:

K-Means is an unsupervised algorithm, but it can be adapted for classification in some cases. After clustering the data, each cluster is assigned a class label based on the majority class of the data points in that cluster. Then, new data points are classified by assigning them to the nearest cluster.

**Example:** In a customer segmentation scenario, K-Means can group customers into clusters based on features like purchasing behavior, and then you can assign each cluster a label such as "High Value" or "Low Value."

##### **When to apply this**:

- When you have an **unsupervised learning** problem and want to group similar data points together without prior knowledge of the labels.
- When the data naturally forms **clusters** or when you believe that the data can be grouped into a small number of categories.
- When computational efficiency is important, as K-Means is relatively fast and simple compared to more complex clustering algorithms.
- When the number of clusters KK is known beforehand or can be estimated using techniques like the **elbow method** or **silhouette score**.
 
##### **Limitations**:

- K-Means assumes that the clusters are **spherical** and equally sized, which may not hold true in real-world data.
- It is sensitive to the initial placement of centroids and can converge to local minima. Using **K-Means++** for initialization can help mitigate this.
- K-Means requires you to specify the number of clusters KK beforehand, which may be difficult if the optimal number of clusters is not obvious.
- K-Means works poorly with **non-globular** clusters or clusters of varying sizes and densities.

---


#### **3. DBSCAN Clustering**

DBSCAN is a density-based clustering algorithm that groups together data points that are closely packed together while marking points that are in low-density regions as outliers (noise). It does not require specifying the number of clusters in advance, unlike K-Means. DBSCAN is particularly effective for identifying clusters of arbitrary shapes and handling noise in the data. DBSCAN was introduced by **Martin Ester**, **Hans-Peter Kriegel**, **Jörg Sander**, and **Xiaowei Xu** in 1996. It was a breakthrough in density-based clustering, offering a solution to the limitations of algorithms like K-Means, which struggle with non-spherical clusters and noise.

##### **Mathematical intuition**: 

The core idea behind DBSCAN is to find regions in the dataset where the density of data points is higher than in the surrounding regions. It uses two key parameters to define this density:

**Epsilon (ϵ\epsilon)**:
The maximum distance between two points for them to be considered neighbors.

**MinPts**:
The minimum number of points required to form a dense region (a cluster).

**DBSCAN classifies points into three categories:**
1. **Core Points**: Points that have at least MinPts points (including itself) within a distance of ϵ\epsilon.
2. **Border Points**: Points that are within $\epsilon$ distance of a core point but do not have enough neighbors to be considered core points.
3. **Noise Points**: Points that are neither core points nor border points; these are outliers or noise.

**Algorithm Steps**:
1. For each unvisited point, find all points within a radius of ϵ\epsilon.
2. If the point has at least MinPtsMinPts neighbors, a new cluster is formed, and the algorithm recursively expands the cluster by visiting all neighbors and their neighbors.
3. If the point has fewer than MinPtsMinPts neighbors, it is labeled as noise.
4. The algorithm repeats the process for all points until all points are visited.

##### **How it is used in regression**:

DBSCAN is not directly used for regression tasks, as it is a clustering algorithm. However, in regression tasks, DBSCAN can be used for **preprocessing** to identify outliers. By clustering data into regions with high density, DBSCAN can help remove noisy or outlier data points before applying regression models.

**Example:** In a house price prediction task, DBSCAN can identify and remove outliers (e.g., very high or low house prices) that might skew the regression model.

##### **How it is used in classification**:

DBSCAN is an unsupervised learning algorithm, so it is not typically used for direct classification. However, it can be combined with supervised classifiers:
- First, DBSCAN can cluster the data into meaningful groups.
- Then, a classifier (like logistic regression, decision trees, or Naive Bayes) can be trained on the clusters, with each cluster representing a class label.

**Example:** In a customer segmentation task, DBSCAN can identify clusters of customers based on their purchasing behaviors. Then, a classification model can predict customer types based on the clusters.

##### **When to apply this**:

- When you want to detect **arbitrary-shaped clusters**, as DBSCAN is capable of finding clusters of any shape, unlike K-Means which assumes spherical clusters.
- When your data has **noise and outliers**, as DBSCAN naturally identifies outliers and does not include them in the clustering process.
- When you **do not know the number of clusters** in advance. DBSCAN does not require you to specify the number of clusters (unlike K-Means).
- When the data is **non-linear** or has varying densities, as DBSCAN can handle clusters of different densities.

##### **Limitations**:

- **Choice of ϵ\epsilon and MinPts**: The performance of DBSCAN heavily depends on the choice of ϵ\epsilon and MinPtsMinPts. Selecting appropriate values can be challenging, especially if the data has varying densities.
- **Difficulty with varying densities**: If the data has clusters with significantly different densities, DBSCAN may struggle to correctly classify all points. This can be mitigated by using algorithms like **HDBSCAN** (Hierarchical DBSCAN), which is an extension of DBSCAN that can handle varying densities.
- **Scalability**: DBSCAN can be computationally expensive for very large datasets, as it requires computing pairwise distances for all points.

#### **4. Hierarchical Clustering**

Hierarchical clustering is an unsupervised machine learning algorithm that builds a hierarchy of clusters. It does this by either recursively merging smaller clusters into larger ones (**agglomerative clustering**) or by recursively dividing larger clusters into smaller ones (**divisive clustering**). The result is often represented as a tree-like diagram called a **dendrogram**, which shows the merging or splitting process and helps visualize the cluster structure. The concept of hierarchical clustering has been around for decades. The first formal methods were introduced in the early 20th century, with significant contributions from **Robert Sokal** and **Peter Michener** in 1958, who helped formalize hierarchical clustering methods for biological taxonomy.

##### **Mathematical intuition**: 

In hierarchical clustering, the algorithm starts with each data point as its own cluster (in agglomerative clustering) or treats the entire dataset as a single cluster (in divisive clustering). The algorithm then iteratively merges or splits clusters based on a distance metric, such as Euclidean distance, and a linkage criterion.

**Agglomerative Clustering**:
- Start with each point as a separate cluster.
- At each step, merge the two closest clusters based on a distance metric (e.g., Euclidean distance).
- The process continues until all data points belong to one cluster.

**Divisive Clustering**:
- Start with all points in a single cluster.
- Iteratively split the clusters based on a distance metric until each point is in its own cluster.

**Linkage Criteria**:
The method for determining the "distance" between clusters affects the resulting hierarchy. 
Common linkage methods include:
- **Single Linkage**: The shortest distance between any two points in different clusters.
- **Complete Linkage**: The longest distance between any two points in different clusters.
- **Average Linkage**: The average distance between all pairs of points in two different clusters.
- **Ward’s Linkage**: Minimizes the variance within the merged clusters.

##### **How it is used in regression**:

Hierarchical clustering is not typically used in regression directly, but it can be used as a preprocessing step to group similar data points, and then separate regression models can be applied to each cluster. By clustering the data first, regression can focus on more homogeneous groups, improving accuracy.

**Example:** Predicting customer lifetime value by first using hierarchical clustering to group customers with similar behaviors, and then applying regression to predict the value for each cluster.

##### **How it is used in classification**:

Hierarchical clustering is an unsupervised method, but it can aid classification by grouping similar instances and then assigning class labels to these groups. You can label clusters by assigning the majority class of the points within the cluster. New data points are classified based on their proximity to existing clusters.

**Example:** In a disease classification scenario, you could use hierarchical clustering to group patients with similar symptoms and then assign a label (e.g., disease type) to each cluster.

##### **When to apply this**:

- When you want to create a **hierarchical structure** of clusters rather than just flat groupings.
- When the number of clusters is not known ahead of time and you want to explore different levels of granularity.
- When data has a natural hierarchical relationship, such as in **gene expression data**, **taxonomy classification**, or **document clustering**.
- When interpretability is important, as hierarchical clustering provides a clear dendrogram that helps understand the data structure and the relationships between clusters.

##### **Limitations**:

- **Computationally expensive**: Hierarchical clustering can be slow for large datasets due to its $O(n^3)$ complexity in the naive version. Optimized algorithms can reduce this, but it still may not scale well for very large datasets.
- **Sensitivity to noise and outliers**: Hierarchical clustering is sensitive to noisy data and outliers, as they can disproportionately affect the distance calculations and cluster assignments.
- **Difficult to scale**: While hierarchical clustering can work well for smaller datasets, its time complexity makes it less practical for large datasets compared to other clustering algorithms like K-Means.
- **No clear number of clusters**: Unlike K-Means, which requires you to specify the number of clusters in advance, hierarchical clustering requires an additional step of cutting the dendrogram to determine the final number of clusters.

---

# **Summary**

Unsupervised learning is a machine learning technique where models learn from unlabeled data to find hidden patterns, clusters, or relationships. It is mainly used for tasks like clustering, anomaly detection, and dimensionality reduction. Common algorithms include Principal Component Analysis (PCA), K-Means clustering, DBSCAN, and hierarchical clustering, each serving different purposes like reducing data complexity, grouping similar data points, or detecting outliers. These methods are widely applied in fields like image recognition, customer segmentation, and market research, where predefined labels aren't available.

---