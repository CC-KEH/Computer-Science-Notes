# K-Nearest Neighbors

K-Nearest Neighbors (KNN) is a simple, instance-based, and non-parametric supervised learning algorithm used for classification and regression tasks. It makes predictions based on the **k** closest training data points to the input data. KNN doesn't build a model upfront but stores the entire training dataset and makes predictions by comparing the input to the stored examples.
KNN was introduced by **[Fix](https://en.wikipedia.org/wiki/Evelyn_Fix) and [Hodges](https://en.wikipedia.org/wiki/Joseph_Lawson_Hodges_Jr.)** in 1951 and has evolved to become one of the most widely used algorithms in machine learning, especially for classification tasks.

## **1. Introduction**

- **K-Nearest Neighbors (KNN)** is a **non-parametric, instance-based (lazy) learning algorithm** used for **classification and regression**.
- It **stores** the training data and makes predictions by finding the **K nearest neighbors** of a given test point.
- **Common Use Cases**: Image recognition, recommendation systems, anomaly detection.

## **2. Working of KNN**

1. Choose a value for **K** (number of neighbors).
2. Calculate the **distance** between the test data point and all training points.
3. Select the **K closest neighbors**.
4. **Classification**: Assign the most common class among the K neighbors (**majority voting**).
5. **Regression**: Compute the average (or weighted average) of the K neighbors' values.

## **3. Distance Metrics**

The choice of **distance metric** is crucial for KNN performance.

#### **(i) Euclidean Distance (Most Common)**

$$d(x, y) = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}$$

- Works well in **low-dimensional** space.
- **Sensitive to scale differences** → Requires **feature scaling** (MinMax or Standardization).

#### **(ii) Manhattan Distance (L1 Norm)**

$$d(x, y) = \sum_{i=1}^{n} |x_i - y_i|$$

- Useful for grid-based problems (like **chessboard moves**).
- Less affected by **outliers** than Euclidean.

#### **(iii) Minkowski Distance (Generalized Form)**

$$d(x, y) = \left( \sum_{i=1}^{n} |x_i - y_i|^p \right)^{\frac{1}{p}}$$

- If **p = 1**, it's **Manhattan Distance**.
- If **p = 2**, it's **Euclidean Distance**.

#### **(iv) Hamming Distance**

- Used for **categorical variables**.
- Counts the number of different characters.

## **4. Choosing the Value of K**

**Small K (e.g., K=1, K=3):**
- More sensitive to **outliers**.
- High variance, may lead to **overfitting**.

**Large K (e.g., K=15, K=20):**
- Reduces noise but may cause **underfitting**.
- More computationally expensive.

**Best Practice**: Choose $K$ using **cross-validation** or set $K = \sqrt{N}$ (where $N$ is the number of training samples).

## **5. Complexity**

**Training Time Complexity:**
- $O(1)$ (No actual training happens).

**Prediction Time Complexity:**
- **$O(N \cdot D)$** (where $N$ is training samples and $D$ is features).

**Limitation**: Slow for **large datasets**.

## **6. Pros & Cons of KNN**

#### **Advantages**

- **Simple & Easy to Implement**  
- **No Training Required (Lazy Learning)**  
- **Can be used for Classification & Regression**  
- **Non-parametric (No Assumptions about Data Distribution)**

#### **Disadvantages**

- **Computationally Expensive** (Slow for large datasets)  
- **Sensitive to Irrelevant Features**  
- **Affected by Imbalanced Data**  
- **Curse of Dimensionality** – High dimensions reduce accuracy

## **7. Applications of KNN**

- **Handwritten Digit Recognition** (MNIST dataset)
- **Recommendation Systems** (Movie suggestions, Collaborative Filtering)
- **Anomaly Detection** (Fraud detection)
- **Medical Diagnosis** (Predicting diseases based on symptoms)

---

# **K-Nearest Neighbors (KNN) for Regression and Classification**

KNN can be used for **both classification and regression** by adjusting how it calculates predictions based on the neighbors' values.

---

## **1. KNN for Classification**

- The **majority class** among the **K nearest neighbors** determines the predicted class.
- Uses **"majority voting"**:

$$\hat{y} = \text{argmax} \sum_{i \in K} I(y_i = c)$$

where:
- $\hat{y}$ is the predicted class.
- $I(y_i = c)$ is an indicator function that counts occurrences of class $c$ in the K neighbors.

#### **Weighted KNN for Classification**

- **Weighted Voting:** Closer neighbors have **higher influence** than distant ones.
- Assigns a weight $w_i$ to each neighbor:

$$\hat{y} = \arg\max_c P(c)$$

$$P(c) = \sum_{i \in K} w_i \cdot I(y_i = c)$$

$$w_i = \frac{1}{d(x, x_i) + \epsilon}$$

- $d(x, x_i)$ is the distance between test point $x$ and training point $x_i$.
- **Closer neighbors contribute more** to the final prediction.

---

## **2. KNN for Regression**

- Instead of classification, KNN can **predict continuous values**.
- Takes the **average (or weighted average)** of the **K nearest neighbors**.

$$hat{y} = \frac{1}{K} \sum_{i=1}^{K} y_i$$

where $y_i$ are the target values of the K closest neighbors.

#### **Weighted KNN for Regression**

- Instead of a simple average, **closer points get higher weight**:

$$\hat{y} = \frac{\sum_{i=1}^{K} w_i y_i}{\sum_{i=1}^{K} w_i}$$

where: 

$$w_i = \frac{1}{d(x, x_i) + \epsilon}$$

- **Closer points contribute more to the prediction**.

#### **When to Use Weighted KNN?**

- **When distances vary significantly**: Weighting helps if some neighbors are much closer.
- **When fewer neighbors are used**: Small K benefits more from weights.

#### **Alternative Weighting Functions**

Sometimes, instead of inverse distance, we use other weight functions:

#### **(i) Gaussian Kernel Weighting**

$$w_i = e^{-\frac{d(x, x_i)^2}{2\sigma^2}}$$

- $\sigma$ = Bandwidth parameter (controls smoothness).
- Assigns **higher** weights to **closer** neighbors.

#### **(ii) Uniform Weighting (Simple KNN)**

$$w_i = 1, \quad \forall i \in K$$

- This is the standard **unweighted KNN** (equal contribution).

---

# **Summary**

K-Nearest Neighbors (KNN) is a simple, non-parametric, instance-based algorithm used for both classification and regression tasks. It makes predictions by identifying the K nearest data points to a given input, without building a model but instead storing the entire training dataset. KNN uses various distance metrics, such as Euclidean, Manhattan, and Minkowski, to measure similarity between points, with the choice of metric being critical for performance. For classification, KNN assigns the most frequent class among the K neighbors, while for regression, it computes the average (or weighted average) of the neighbors' values. The value of K is a crucial hyperparameter—small K values are more sensitive to outliers, whereas large K values might lead to underfitting. KNN is computationally expensive, especially for large datasets, and is sensitive to irrelevant features and the curse of dimensionality. The algorithm finds applications in areas like image recognition, recommendation systems, anomaly detection, and medical diagnosis. Weighted KNN adjusts the influence of neighbors by assigning different weights based on their distance, with options for Gaussian kernel and uniform weighting. KNN's simplicity and versatility make it a popular choice, but it requires careful tuning and efficient implementation for large-scale problems.

---