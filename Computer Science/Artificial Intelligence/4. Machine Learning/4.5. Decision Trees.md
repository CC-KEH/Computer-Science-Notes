# Decision Trees

A **Decision Tree** is a non-linear supervised learning algorithm that splits the data into subsets based on feature values, recursively building a tree structure where each internal node represents a decision based on a feature, and each leaf node represents the outcome or prediction. 

It is widely used for both classification and regression tasks. The concept of decision trees can be traced back to work in machine learning and decision theory, but the modern algorithm was formalized in the 1980s. ID3 (Iterative Dichotomiser 3), an early decision tree algorithm, was introduced by [Ross Quinlan](https://en.wikipedia.org/wiki/Ross_Quinlan) in 1986. Later algorithms, like C4.5, C5.0, and CART (Classification and Regression Trees), evolved from this foundational work.

---

## **1. Decision Tree Basics**

- Each node represents a **feature**.
- Each branch represents a **decision rule**.
- Each leaf node represents a **prediction (class label for classification, value for regression).**

Decision trees use **splitting criteria** to determine the best feature to split on. The common criteria are:

- **Entropy (Information Gain) → Used in ID3 & C4.5 Algorithm**
- **Gini Index → Used in CART Algorithm**
- **Variance Reduction → Used in Regression Trees**

---

## **2. Splitting Criteria for Classification Trees**

#### **(i) Entropy & Information Gain (Used in ID3, C4.5)**

- **Entropy** measures the impurity (uncertainty) of a node.
- The lower the entropy, the **purer** the node.

**Formula for Entropy**
$$H(S) = - \sum_{i=1}^{C} p_i \log_2 p_i$$

where:

- $p_i$ = Proportion of class ii in the node.
- $C$ = Number of classes.

#### Information Gain (IG)

- Measures the **reduction in entropy** after a split.
$$IG(S, A) = H(S) - \sum_{v \in A} \frac{|S_v|}{|S|} H(S_v)
$$
where:

- $S$ = Parent node.
- $S_v$ = Child nodes after splitting on feature AA.
- $H(S)$ = Entropy before split.
- $H(S_v)$ = Entropy of child nodes.
- **Higher IG means better split.**

#### Example Calculation of Entropy

If a dataset has **40 positive** and **60 negative** samples:

$$H(S) = -\left(\frac{40}{100} \log_2 \frac{40}{100} + \frac{60}{100} \log_2 \frac{60}{100} \right) = 0.97$$

After a split into two child nodes:

- **Node 1 (30 Pos, 10 Neg)** → $H_1 = 0.81$
- **Node 2 (10 Pos, 50 Neg)** → $H_2 = 0.72$

Using IG formula:

$$IG = 0.97 - \left( \frac{40}{100} \times 0.81 + \frac{60}{100} \times 0.72 \right) = 0.22$$

Since IG is positive, the split is meaningful. If there are multiple features, and all have positive IG, then **we select the one with the highest IG**.

#### **(ii) Gini Index (Used in CART Algorithm)**

- Measures how **impure** a dataset is on the scale of 0 to 1.
- Unlike entropy, it **does not use logarithm** (faster computation).

**Formula for Gini Index**

$$Gini(S) = 1 - \sum_{i=1}^{C} p_i^2$$

where:

- $p_i$ = Proportion of class ii in the node.
- **Lower Gini means purer nodes**.

#### Gini Gain

- Similar to **Information Gain**, but based on Gini:

$$GG(S, A) = Gini(S) - \sum_{v \in A} \frac{|S_v|}{|S|} Gini(S_v)
$$
- **Higher Gini Gain = Better Split**.

#### Example Calculation of Gini Index

If a dataset has **40 positive** and **60 negative** samples:

$$Gini(S) = 1 - \left( \left(\frac{40}{100}\right)^2 + \left(\frac{60}{100}\right)^2 \right) = 0.48$$

If a split produces:
- **Node 1 (30 Pos, 10 Neg)** → Gini1=0.42Gini_1 = 0.42
- **Node 2 (10 Pos, 50 Neg)** → Gini2=0.36Gini_2 = 0.36

Using Gini Gain formula:

$$G = 0.48 - \left( \frac{40}{100} \times 0.42 + \frac{60}{100} \times 0.36 \right) = 0.07$$

Since Gini Gain is low, the split is meaningful.

---

## **3. Splitting Criteria for Regression Trees**

Decision trees for regression use **Variance Reduction** instead of Entropy/Gini.

#### **(i) Variance Reduction**

- Measures how much the variance (spread) of the target variable is **reduced** by splitting the data on a particular feature.
- A good split **minimizes the variance within each child node**, making the target values more similar.

**Formula for Variance**

$$Var(S) = \frac{1}{N} \sum_{i=1}^{N} (y_i - \bar{y})^2$$

where:

- $N$ = Number of samples.
- $y_i$ = Target values.
- $\bar{y}$ = Mean of target values.

#### Variance Reduction Formula

$$VR(S, A) = Var(S) - \sum_{v \in A} \frac{|S_v|}{|S|} Var(S_v)$$

- $S$: Parent node sample set.
- $A$: Partition of $S$ into subsets $S_v$​ based on the split.
- $|S_v|$: Number of samples in subset $v$.
- Higher $VR$ means the split reduces variance effectively and is preferred.

#### Example Calculation of Variance

Consider 5 target values: **\[5, 10, 15, 20, 25]**

$$Var(S) = \frac{(5-15)^2 + (10-15)^2 + (15-15)^2 + (20-15)^2 + (25-15)^2}{5} = 50$$

- After splitting into **\[5, 10]** and **\[15, 20, 25]**:
- **Var1 = 6.25, Var2 = 16.67**

$$VR = 50 - \left( \frac{2}{5} \times 6.25 + \frac{3}{5} \times 16.67 \right) = 30.83$$

Since variance reduction is **high**, the split is meaningful.

#### **(ii) Mean Absolute Error (MAE) Reduction**

- Instead of minimizing squared errors (variance), minimize the **mean absolute error** within splits.
- More robust to outliers since it doesn’t square the residuals.
- Less common due to optimization difficulties (MAE is not differentiable everywhere).

#### **(iii) Reduction in Residual Sum of Squares (RSS)**

- Equivalent to variance reduction but expressed as the **sum of squared residuals** instead of average variance.
- Same practical effect; commonly used in implementations.

#### **(iv) Quantile Regression Splits**

- For predicting conditional quantiles (e.g., median), splits can be chosen to minimize quantile loss.
- Used in specialized regression trees for quantile regression.

#### **(v) Poisson Deviance or Other Loss Functions**

- In some specialized regression trees (e.g., for count data or specific distributions), splitting criteria can be based on likelihood or deviance measures tailored to the data distribution.

---

## **4. Advantages & Disadvantages of Decision Trees**

#### **Advantages**

- **Easy to interpret & visualize** 
- **Handles both classification & regression**  
- **No need for feature scaling**  
- **Works with missing values**

#### **Disadvantages**

 - **Prone to overfitting** (Solution: Pruning)  
 - **Sensitive to small changes in data**  
 - **Biased towards dominant classes in imbalanced datasets**

---
## **5. Entropy vs Gini Impurity**

| **Criteria**                 | **Entropy**                                                        | **Gini Impurity**                                                       |
| ---------------------------- | ------------------------------------------------------------------ | ----------------------------------------------------------------------- |
| **Definition**               | Measures **impurity** using logarithmic probabilities.             | Measures **impurity** based on squared probabilities.                   |
| **Formula**                  | $H(S) = - \sum p_i \log_2 p_i$                                     | $Gini(S) = 1 - \sum p_i^2$                                              |
| **Value Range**              | $[0, \log_2 C]$<br>$$ (0 = pure, higher = more\ impurity)$$<br>    | $[0, 0.5]$<br>$$(0 = pure, 0.5 = max\ impurity\ for\ binary\ classes)$$ |
| **Impurity Meaning**         | Higher entropy means **more disorder** in the class distribution.  | Higher Gini means **more impurity** (less homogeneity).                 |
| **Computational Complexity** | **Slower** due to logarithmic calculations.                        | **Faster** (no logarithms).                                             |
| **Best Split Criterion**     | Prefers **balanced** splits (when classes are evenly distributed). | Prefers **dominant** class splits (biased towards larger groups).       |
| **Usage**                    | Used in **ID3, C4.5** decision tree algorithms.                    | Used in **CART (Classification and Regression Trees)** algorithm.       |
| **Bias**                     | **Less biased** towards dominant classes.                          | **More biased** towards larger classes.                                 |

---

# **Summary**

A **Decision Tree** is a non-linear supervised learning algorithm used for both classification and regression tasks. It recursively splits the data into subsets based on feature values, forming a tree structure where internal nodes represent decisions (based on features) and leaf nodes represent predictions (class labels for classification or target values for regression). Decision Trees use splitting criteria like **Entropy (Information Gain)** for classification tasks (used in ID3 and C4.5) and **Gini Index** (used in CART) to measure the impurity of a node, with the goal of selecting the best feature to split on. For regression tasks, **Variance Reduction** is used to minimize the variance within each split. While Decision Trees are easy to interpret and visualize, they are prone to **overfitting** and are sensitive to small data changes. They don’t require feature scaling, can handle missing values, and work well with both categorical and continuous data. However, they are biased towards dominant classes in imbalanced datasets and can become complex and overfit if not pruned. The choice between **Entropy** and **Gini Index** depends on computational efficiency and the specific application, with **Gini Index** being faster and more biased towards larger classes, while **Entropy** prefers more balanced splits.

---
