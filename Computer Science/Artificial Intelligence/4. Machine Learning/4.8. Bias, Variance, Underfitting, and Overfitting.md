# Bias, Variance, Underfitting, and Overfitting

## **1. Bias-Variance Tradeoff**

The **Bias-Variance tradeoff** explains how model complexity affects **error**.

- **Bias**: Error due to overly simplistic assumptions in the learning algorithm.
- **Variance**: Error due to the model being too sensitive to training data.

The **total error** of a model can be decomposed as:

$$Total\ Error = Bias^2 + Variance + Irreducible\ Error$$

#### **Low vs High Bias & Variance**

|Model Type|**Bias**|**Variance**|**Performance**|
|---|---|---|---|
|**Underfitting Model**|**High**|Low|Poor accuracy (too simple)|
|**Good Model**|Moderate|Moderate|Balanced (optimal tradeoff)|
|**Overfitting Model**|Low|**High**|Poor generalization (too complex)|

---

## **2. Bias**

#### **Definition**:

Bias measures how much the model's predictions **systematically differ** from the true values.

#### **High Bias (Underfitting):**
 
- Model is **too simple** and cannot capture underlying patterns.
- Ignores important features or relationships.
- Example: **Linear Regression on non-linear data**.

#### **Low Bias (Overfitting):**

- Model captures training data **too well**, including noise.
- Example: **Deep Neural Networks without regularization**.

#### **Effects of High Bias**

- Poor training performance.  
- Poor test performance (high error).  
- Cannot capture complex relationships.

#### **Reducing Bias**

- Use a **more complex model**.  
- Feature engineering (adding relevant features).  
- Reduce **regularization** (e.g., lower L1/L2 penalties).

---

## **3. Variance**

#### **Definition**:

Variance measures how much the model's predictions **change** with different training datasets.

#### **High Variance (Overfitting)**:

- Model learns the training data **too well**, including noise.
- Performs **well on training** but **poorly on test data**.
- Example: **Decision Trees with deep splits**.

#### **Low Variance (Underfitting)**:

- Model generalizes well but may be **too simple**.
- Example: **Logistic Regression on complex classification tasks**.

#### **Effects of High Variance**

- Model is too sensitive to small changes in training data.  
- Poor test performance (high generalization error).  
- May fail on unseen data.

#### **Reducing Variance**

- Use **simpler models** (e.g., reducing depth in Decision Trees).  
- **Regularization (L1/L2)** to prevent overfitting.  
- Increase **training data** (reduces sensitivity to noise).  
- **Ensemble methods** (e.g., Bagging, Boosting).

---

## **4. Underfitting vs Overfitting**

|**Aspect**|**Underfitting**|**Overfitting**|
|---|---|---|
|**Definition**|Model is **too simple** and fails to capture the pattern.|Model is **too complex** and captures noise along with the pattern.|
|**Bias**|**High**|**Low**|
|**Variance**|**Low**|**High**|
|**Training Error**|**High**|**Low**|
|**Test Error**|**High**|**High** (due to poor generalization)|
|**Model Complexity**|**Too low**|**Too high**|
|**Generalization**|Poor|Poor|
|**Example Models**|Linear Regression on non-linear data, Na√Øve Bayes|Deep Neural Networks without regularization, Overly complex Decision Trees|
|**Solution**|Increase model complexity, add features|Reduce model complexity, regularization, more data|

#### **Illustration**

- **Underfitting**: The model is too simple and does not learn enough from the data.
- **Overfitting**: The model learns too much, including noise, making it unreliable for unseen data.

---

# **Summary**

Bias, variance, underfitting, and overfitting are crucial concepts for understanding model behavior and performance. The **Bias-Variance tradeoff** shows how model complexity influences errors: high bias leads to underfitting, while high variance causes overfitting. Bias refers to errors from overly simplistic assumptions, while variance reflects sensitivity to training data. Underfitting occurs when the model is too simple, leading to poor accuracy, while overfitting happens when the model is too complex and captures noise. To address these issues, we adjust model complexity, use regularization, or implement ensemble methods to find a balance between bias and variance.

---
