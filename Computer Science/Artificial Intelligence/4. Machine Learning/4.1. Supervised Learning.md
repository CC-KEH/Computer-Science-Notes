# **Supervised Learning – Regression**
---

## **What is Supervised Learning?**

- Data consists of **input-output pairs**
- Goal: Learn a mapping **f(x) → y** from training data
- Examples:
    - Predicting house prices (regression)
    - Classifying emails as spam (classification)

---

## **Regression vs Classification**

| Feature    | Regression             | Classification           |
| ---------- | ---------------------- | ------------------------ |
| Output     | Continuous             | Categorical (discrete)   |
| Example    | Predict house price    | Detect spam email        |
| Algorithms | Linear Regression, SVR | Logistic Regression, SVM |
| Evaluation | MSE, MAE, R²           | Accuracy, F1 Score, AUC  |

---

## Linear Regression

#### **1. Introduction**

Linear Regression is a fundamental statistical method used in machine learning to model the relationship between a dependent variable (target) and one or more independent variables (features). It assumes a linear relationship between input and output.

**Linear Regression** was formalized by [Sir Francis Galton](https://en.wikipedia.org/wiki/Francis_Galton) and later refined by [Karl Pearson](https://en.wikipedia.org/wiki/Karl_Pearson) and others. **Logistic Regression** was introduced by [David R. Cox](https://en.wikipedia.org/wiki/David_Cox_(statistician)) in 1958, as a method for modeling binary outcomes.

#### Types of Linear Regression:

- **Simple Linear Regression**: One independent variable.
- **Multiple Linear Regression**: Multiple independent variables ( In same record ).

#### **2. Mathematical Intuition**

Linear Regression models the relationship between input xx and output $y$ using the equation of a straight line:

$$y=mx+c$$

where:

- $y$ = dependent variable (output)
- $x$ = independent variable (input)
- $m$ = slope (weight coefficient)
- $c$ = intercept (bias)

For multiple variables:

$$y = w_1x_1 + w_2x_2 + \dots + w_nx_n + c$$

or in vector notation:

$$\mathbf{y} = \mathbf{Xw} + c$$

where:

- $\mathbf{X}$ is the matrix of features,
- $\mathbf{w}$ is the weight vector.

#### **3. Formula to Find m (Slope) and c (Intercept)**

The values of m and c are determined by minimizing the error between predicted values and actual values.

The **Least Squares Estimation** method provides:

$$m = \frac{n \sum xy - \sum x \sum y}{n \sum x^2 - (\sum x)^2} $$
$$c = \frac{\sum y - m \sum x}{n}$$

For multiple linear regression, we use **Normal Equation**:

$$\mathbf{w} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}$$

where:

- $\mathbf{X}^T$ is the transpose of the feature matrix,
- $\mathbf({X}^T \mathbf{X})^{-1}$ is the inverse.

#### **4. Loss Function (Cost Function)**

To measure how well the model predicts, we use the **Mean Squared Error (MSE)**:

$$J(m, c) = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$

where:

- $J(m, c)$ is the cost function,
- $y_i$ is the actual value,
- $\hat{y}_i$ is the predicted value.

#### **Gradient Descent** (Optimization Method):


To minimize $J(m, c)$, we iteratively update $m$ and $c$ using:

$$m := m - \alpha \frac{\partial J}{\partial m} $$
$$c := c - \alpha \frac{\partial J}{\partial c}$$
where:
- $\alpha$ is the learning rate.

#### **5. Complexity**

**Training Complexity**:
- **Normal Equation**: $O(n^2)$ or $O(nd^2)$
    - This involves solving a closed-form matrix equation $(X^T X)^{-1} X^T y$, where $X$ is the feature matrix of size $n×d$.
    - Matrix inversion has a complexity of $O(d^3)$, and matrix multiplication is $O(nd^2)$.
- **Gradient Descent**: $O(nd)$ per iteration
    - The cost function gradient computation takes $O(nd)$, and multiple iterations are required.
**Prediction Complexity**: $O(d)$
- A new data point is predicted using $w^T x$, which is a dot product operation of $O(d)$.

#### **6. Advantages of Linear Regression**

- **Simple & Interpretable**: Easy to understand and explain.
- **Computationally Efficient**: Works well for small to medium datasets.
- **Closed-Form Solution**: Can be solved using matrix operations.
- **Good for Linearly Separable Data**: Works well when the data has a linear relationship.

#### **7. Disadvantages of Linear Regression**

- **Assumption of Linearity**: Doesn't work well for non-linear relationships.
- **Sensitive to Outliers**: Large deviations can heavily impact predictions.
- **Feature Dependence**: Requires careful feature engineering and scaling.
- **Collinearity Issues**: Highly correlated features can distort coefficients.

#### **8. Assumptions of Linear Regression**

For Linear Regression to provide reliable results, the following assumptions must hold:

#### 1. Linearity of Relationship

- The relationship between the independent variable(s) x and the dependent variable y must be linear.
- This means that increasing x leads to a proportional change in y.
- If the relationship is non-linear, transformations (e.g., log, polynomial regression) may be needed.

#### 2. Independence of Errors (No Autocorrelation)

- The residuals (errors) should not be correlated with each other.
- This is particularly important for **time series data**, where errors might be dependent over time.
- **Durbin-Watson test** can be used to detect autocorrelation.

#### 3. Homoscedasticity (Constant Variance of Errors)

- The variance of residuals (errors) should remain constant across all levels of the independent variable.
- If variance is not constant (heteroscedasticity), predictions might be inefficient.
- **Breusch-Pagan test** is used to check for heteroscedasticity.

#### 4. Normality of Errors

- The residuals should be **normally distributed** (bell-shaped curve).
- This assumption is important for hypothesis testing and confidence intervals.
- **Shapiro-Wilk test** or **Q-Q plots** can check for normality.

#### 5. No Multicollinearity (For Multiple Linear Regression)

- Independent variables should not be highly correlated with each other.
- If multicollinearity exists, it distorts coefficient estimates.
- **Variance Inflation Factor (VIF)** is used to detect multicollinearity.

#### 6. No Endogeneity (Independent Variables Are Not Correlated with Errors)

- The independent variables should not be correlated with the error term.
- If violated, it leads to **biased and inconsistent estimates**.

---

## Polynomial Regression

#### **1. Introduction**

Polynomial Regression is an extension of **Linear Regression** where the relationship between the independent variable xx and the dependent variable yy is modeled as an **n-th degree polynomial** instead of a straight line. It is useful when data shows a **non-linear relationship** that can still be approximated using polynomial terms.

Polynomial Regression was developed as an enhancement to **Linear Regression**, allowing models to fit more complex patterns without requiring entirely new algorithms.

#### **2. Mathematical Intuition**

Polynomial Regression extends **Linear Regression** by introducing polynomial features:

$$y = w_0 + w_1x + w_2x^2 + w_3x^3 + \dots + w_nx^n$$

where:

- $y$ = dependent variable (output)
- x = independent variable (input)
- $w_0, w_1, w_2, \dots, w_n$ = coefficients (weights)
- $n$ = degree of the polynomial

For multiple variables:

$$y = w_0 + w_1x_1 + w_2x_1^2 + w_3x_1^3 + \dots + w_nx_1^n + w_{n+1}x_2 + w_{n+2}x_2^2 + \dots$$

or in **vector notation**:

$$\mathbf{y} = \mathbf{Xw} + c$$

where:

- $\mathbf{X}$ is the transformed feature matrix, including polynomial terms.
- $\mathbf{w}$ is the weight vector.

#### **3. Transforming Features for Polynomial Regression**

Since Polynomial Regression is still solved as a **linear model**, we transform input features using **Polynomial Feature Expansion**.

For a single variable xx, the transformation:

$$X = \begin{bmatrix} 1 & x_1 & x_1^2 & x_1^3 & \dots & x_1^n \\ 1 & x_2 & x_2^2 & x_2^3 & \dots & x_2^n \\ \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\ 1 & x_m & x_m^2 & x_m^3 & \dots & x_m^n \end{bmatrix}$$

This allows **Linear Regression** techniques to be applied to learn polynomial coefficients.

#### **4. Loss Function (Cost Function)**

Polynomial Regression uses the **Mean Squared Error (MSE)** as its cost function:

$$J(w) = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$

where:

- $J(w)$ is the cost function.
- $y_i$ is the actual value.
- $\hat{y}_i$ is the predicted value.

To minimize $J(w)$, we use:

- **Gradient Descent**
- **Normal Equation**

#### **5. Formula for Finding Coefficients**

Like Linear Regression, Polynomial Regression finds the optimal **weight vector** ww using the **Normal Equation**:

$$\mathbf{w} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}$$

where:

- $\mathbf{X}^T$ is the transpose of the feature matrix.
- $(\mathbf{X}^T \mathbf{X})^{-1}$ is the inverse.

Alternatively, **Gradient Descent** updates the coefficients iteratively:

$$w_j := w_j - \alpha \frac{\partial J}{\partial w_j}$$


#### **6. Complexity**

- **Training Complexity**:
    - **Normal Equation**: $O(n^2)$ to $O(nd^2)$
    - **Gradient Descent**: $O(nd)$ per iteration
- **Prediction Complexity**: $O(d)$, since evaluation involves a polynomial equation.

#### **7. Advantages of Polynomial Regression**

- **Captures Non-Linearity**: Unlike Linear Regression, it models curved relationships.  -
- **Simple & Easy to Implement**: Uses standard regression techniques.  
- **More Flexible than Linear Regression**: Can approximate complex patterns.  
- **Still Interpretable**: Unlike neural networks, it remains mathematically explainable.

#### **8. Disadvantages of Polynomial Regression**

- **Overfitting Risk**: Higher-degree polynomials can capture noise rather than the actual trend.  
- **Extrapolation is Unreliable**: Predictions outside the training range can behave unpredictably.  
- **Computationally Expensive**: High-degree polynomials require more computation.  
- **Feature Scaling Required**: Without proper scaling, polynomial terms can dominate.

#### **9. Assumptions of Polynomial Regression**

Polynomial Regression follows similar assumptions as **Linear Regression**, with some modifications:

#### 1. Non-Linearity is Present

- The relationship between the independent variable(s) and the dependent variable should follow a **polynomial trend** (e.g., quadratic, cubic).
- Applying polynomial regression to data without such trends may lead to **overfitting** or misleading results.

#### 2. Independence of Errors (No Autocorrelation)

- Residuals (errors) should be **independent** of each other.
- Especially critical for **time series or sequential data**, where autocorrelation can violate this assumption.

#### 3. Homoscedasticity (Constant Variance of Errors)

- The variance of residuals should be **constant across all values** of the independent variable(s).
- If variance changes (heteroscedasticity), it may affect the reliability of confidence intervals and hypothesis tests.

#### 4. Normality of Errors

- Residuals should be approximately **normally distributed**.
- This assumption is important for valid inference, such as constructing confidence intervals and performing hypothesis tests.

#### 5. No Perfect Multicollinearity

- Polynomial terms (e.g., x, x², x³) are inherently correlated, leading to **multicollinearity**.
- High multicollinearity can inflate variance of coefficient estimates, making them unstable and hard to interpret.
- **Feature selection, regularization, or orthogonal polynomials** can help mitigate this issue.

---

# **Summary**

Linear Regression is a foundational statistical technique used to model the relationship between one or more independent variables and a continuous dependent variable, assuming a straight-line relationship. Its core lies in minimizing the difference between predicted and actual values using methods like the Least Squares Estimation or Gradient Descent. When data exhibits curvature rather than linearity, Polynomial Regression extends this model by introducing polynomial terms, allowing for better fitting of non-linear trends while still using linear model frameworks. Both Linear and Polynomial Regression assume conditions like linearity (or transformable non-linearity), independence, constant variance, and normality of residuals.

---
