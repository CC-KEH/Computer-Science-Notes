## **Introduction to Classification**

#### **What is classification?**

Classification is a supervised learning task where the goal is to predict a **categorical label** (class) for given input data. The model learns from labeled examples to assign new data points into one of several predefined classes.

#### **Difference between regression and classification**

| Aspect                 | Regression                           | Classification                                  |
| ---------------------- | ------------------------------------ | ----------------------------------------------- |
| **Output Type**        | Continuous numeric value             | Discrete category/class label                   |
| **Goal**               | Predict a quantity or value          | Predict the class/category                      |
| **Examples**           | Predicting house prices, temperature | Predicting spam vs. not spam, disease diagnosis |
| **Evaluation Metrics** | MSE, RMSE, MAE                       | Accuracy, Precision, Recall, F1 Score           |

#### **Types of classification**

- **Binary Classification:** Distinguish between two classes (e.g., spam or not spam).
- **Multi-class Classification:** Classify data points into more than two classes (e.g., types of fruits: apple, orange, banana).
- **Multi-label Classification:** Assign multiple labels to each instance (e.g., tagging an image with multiple objects: cat, tree, car).

#### **Real-world examples:**

- **Spam Detection:** Classify emails as spam or not spam.
- **Email Categorization:** Organize emails into folders like Promotions, Social, Updates.
- **Disease Prediction:** Diagnose diseases based on patient symptoms and medical data.

---

## **Logistic Regression (Binary Classification)**

#### **1. Introduction**

Logistic Regression is a **classification algorithm** used to predict categorical outcomes (e.g., binary classification: Yes/No, 0/1). It is based on **the logistic (sigmoid) function**, which maps any real-valued number to a probability between **0 and 1**.

- **Types of Logistic Regression:**
    - **Binary Logistic Regression** → Two classes (0 or 1)
    - **Multinomial Logistic Regression** → More than two classes (unordered)
    - **Ordinal Logistic Regression** → More than two classes (ordered)

#### **2. Mathematical Intuition**

In **Linear Regression**, we predict $y$ using:

$$y = w_1x_1 + w_2x_2 + \dots + w_nx_n + c$$

However, linear regression is unsuitable for classification since it does not produce probability values.

**Solution:** Instead of predicting $y$ directly, we apply the **sigmoid function**:

$$P(y=1 | x) = \sigma(z) = \frac{1}{1 + e^{-z}}$$

where:

- $\sigma(z)$ is the **sigmoid function**
- $z = w_1x_1 + w_2x_2 + \dots + w_nx_n + c$ is the linear combination of inputs
- The output is interpreted as a **probability (between 0 and 1)**

**Decision Rule:**

$$y = \begin{cases} 1, & \text{if } P(y=1 | x) \geq 0.5 \\ 0, & \text{if } P(y=1 | x) < 0.5 \end{cases}$$

For **multiclass classification**, **Softmax Regression** is used instead of the sigmoid function.

#### **3. Formula for Model Parameters w (Weights)**

Unlike Linear Regression, we cannot directly compute weights using the Normal Equation. Instead, we use **Maximum Likelihood Estimation (MLE)**.

#### **Likelihood Function**

Since Logistic Regression outputs probabilities, we define the **likelihood function** as:

$$L(w) = \prod_{i=1}^{n} P(y_i | x_i)$$

Taking the log for computational ease, we get the **Log-Likelihood function**:

$$\log L(w) = \sum_{i=1}^{n} \left[ y_i \log P(y_i) + (1 - y_i) \log (1 - P(y_i)) \right]$$

To estimate $w$, we **maximize** this log-likelihood function using **Gradient Descent**.

#### **4. Loss Function (Cost Function)**

The loss function used in Logistic Regression is the **Binary Cross-Entropy (Log Loss)**:

$$J(w) = - \frac{1}{n} \sum_{i=1}^{n} \left[ y_i \log \hat{y}_i + (1 - y_i) \log (1 - \hat{y}_i) \right]$$

where:

- $\hat{y}_i$ is the predicted probability $P(y=1 | x)$.

**Gradient Descent Update Rule:**

$$w := w - \alpha \frac{\partial J}{\partial w}$$
$$c := c - \alpha \frac{\partial J}{\partial c}$$

where:

- $\alpha$ = learning rate
- $w_j$ = weight for feature $x_j$
- $c$ = bias term
- $\hat{y}_i - y_i$ = error term

#### **5. Complexity Analysis**

- **Training Complexity**: $O(nd)$ per iteration
    - Similar to Linear Regression but requires iterative optimization like Gradient Descent.
- **Prediction Complexity**: $O(d)$
    - Uses a sigmoid activation function, which requires a dot product $O(d)$.

#### **6. Assumptions of Logistic Regression**

For Logistic Regression to work effectively, the following assumptions must hold:

##### 6.1. Linearity of Log-Odds (Logit Transformation)

- The independent variables **do not need to be linearly related** to the dependent variable.
- However, the **log-odds** (logit) must have a linear relationship with independent variables:
$$\left(\frac{P(y=1)}{1 - P(y=1)}\right) = w_1x_1 + w_2x_2 + \dots + w_nx_n + c$$
##### 6.2. Independence of Observations

- Each data point should be **independent** of the others.
- Violation occurs in time-series or grouped data (handled using **Generalized Linear Models**).

##### 6.3. No Multicollinearity

- Independent variables should not be highly correlated.
- **Variance Inflation Factor (VIF)** is used to detect multicollinearity.

##### 6.4. No Extreme Outliers

- Logistic Regression is sensitive to extreme outliers.
- Outliers can be handled using **regularization (L1/L2 penalty)**.

#### **7. Advantages of Logistic Regression**

- **Simple & Interpretable** → Easy to understand and explain.
- **Probability Scores** → Outputs probabilities rather than fixed labels.
- **Efficient for Small Datasets** → Works well when data is limited.
- **No Feature Scaling Required** → Unlike SVM or kNN, no strict need for normalization.
- **Extends to Multiclass Problems** → Using **Softmax Regression**.

#### **8. Disadvantages of Logistic Regression**

- **Assumes Linearity in Log-Odds** → May not work well for non-linear data.
- **Sensitive to Outliers** → Can be influenced by extreme values.
- **Not Suitable for High-Dimensional Data** → Struggles when the number of features is very high (e.g., image classification).
- **Overfitting in Small Data** → Needs **regularization (L1/L2)** when working with many features.

---

## **Multiclass Classification**

Multiclass classification involves assigning an input to one of three or more classes.

#### **Strategies:**

- **One-vs-Rest (OvR):**  
    Train a separate binary classifier for each class, where each classifier distinguishes one class from all others. At prediction, the class with the highest confidence score is selected.
- **Softmax Regression (Multinomial Logistic Regression):**  
    A single model that directly predicts the probabilities of each class simultaneously by applying the softmax function.

#### **Softmax function:**
$$
\text{Softmax}(x_{i}) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}​​
$$

- Converts raw model outputs (logits) into probabilities across all classes that sum to 1.
- Highlights the largest logits while diminishing smaller ones, facilitating probabilistic interpretation.

---

# **Summary**

**Classification – Logistic Regression & Beyond** covers foundational concepts and techniques for solving classification problems in machine learning. It starts with **logistic regression**, a popular method for binary classification that models the probability of class membership using the logistic (sigmoid) function. The topic then extends to **multiclass classification** techniques like One-vs-Rest and Softmax Regression, which handle problems with more than two classes.

---